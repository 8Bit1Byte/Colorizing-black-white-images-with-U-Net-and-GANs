{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Colorization using U-Net and GANs\n",
    "#### 1. Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Importing necessary libraries and Setting Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gs\n",
    "from matplotlib import font_manager as fm, rcParams\n",
    "from PIL import Image\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Fetching The Dataset and Setting Up Input Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing fastai for Quickly Getting The COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U fastai\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing The Dataset in the Following Directory Structure\n",
    "<pre>\n",
    ".\n",
    "└── .fastai\n",
    "    └── data\n",
    "        └── coco_sample\n",
    "            └── train_sample\n",
    "                └── *.jpg (10,000 images in total)</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.external import untar_data, URLs\n",
    "coco_path = untar_data(URLs.COCO_SAMPLE)\n",
    "coco_path = str(coco_path) + \"/train_sample\"\n",
    "paths = glob.glob(coco_path + \"/*.jpg\")\n",
    "# Setting seed for getting the same data across all train sessions \n",
    "np.random.seed(123)\n",
    "paths_subset = np.random.choice(paths, 10_000, replace=False) # choosing 10000 images randomly\n",
    "rand_idxs = np.random.permutation(10_000)\n",
    "train_idxs = rand_idxs[:8000] # choosing the first 8000 as training set\n",
    "val_idxs = rand_idxs[8000:] # choosing last 2000 as validation set\n",
    "train_paths = paths_subset[train_idxs]\n",
    "val_paths = paths_subset[val_idxs]\n",
    "print(train_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previewing The Input Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageCount = 0\n",
    "fig, ax = plt.subplots(4, 4, figsize=(13,13))\n",
    "for i in range(4):\n",
    "    for j in range(4):    \n",
    "        ax[i, j].imshow(Image.open(train_paths[imageCount]))\n",
    "        imageCount+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Defining Train and Test Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageSize = 256\n",
    "class MakeDataset(Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.transforms = transforms.Compose([\n",
    "                transforms.Resize((ImageSize, ImageSize),  transforms.InterpolationMode.BICUBIC),\n",
    "                transforms.RandomHorizontalFlip(), # Added after 350th Epoch to see if Results improves\n",
    "            ])\n",
    "        self.paths=paths\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.paths[i])\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = self.transforms(img)\n",
    "        img = np.array(img)\n",
    "        imgInLAB = rgb2lab(img).astype(\"float32\")\n",
    "        imgInLAB = transforms.ToTensor()(imgInLAB)\n",
    "        L_array = imgInLAB[[0], ...] / 50. - 1.\n",
    "        ab_array = imgInLAB[[1, 2], ...] / 110.\n",
    "        return [L_array, ab_array]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making dataloaders with input images transformed to L and ab image space, after resizing to 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BatchSize, Workers = [16, 4]\n",
    "trainDL = DataLoader(MakeDataset(paths=train_paths), batch_size=BatchSize, num_workers=Workers, pin_memory=True, shuffle = True)\n",
    "validationDL = DataLoader(MakeDataset(paths=val_paths), batch_size=BatchSize, num_workers=Workers, pin_memory=True, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the Transformed Data\n",
    "##### Helper Function for Converting a batch of Lab images into a batch of RGB images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_rgb(L, ab):  \n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(trainDL))\n",
    "L_Array, ab_Array = data[0], data[1]\n",
    "print(f\"L Array Shape : {L_Array.shape}\", f\"*a*b Array Shape : {ab_Array.shape}\",sep='\\n')\n",
    "\n",
    "fig, (ax0, ax1, ax2, ax3) = plt.subplots(1, 4, figsize=(15,10))\n",
    "ax0.imshow(L_Array[0][0], cmap='gray')\n",
    "ax0.set_title('L')\n",
    "ax1.imshow(ab_Array[0][0])\n",
    "ax1.set_title('a')\n",
    "ax2.imshow(ab_Array[0][1])\n",
    "ax2.set_title('b')\n",
    "ax3.imshow(lab_to_rgb(L_Array,ab_Array)[0])\n",
    "ax3.set_title('RGB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Modeling the Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, inputs, outputs, down=True, batchNorm=True, dropout=False):\n",
    "        super(GenBlock,self).__init__()\n",
    "\n",
    "        if down:\n",
    "            self.block1 = nn.Conv2d(inputs, outputs, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            self.block4 = nn.LeakyReLU(0.2, True)\n",
    "        else:\n",
    "            self.block1 = nn.ConvTranspose2d(inputs, outputs, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            self.block4 = nn.ReLU(True)\n",
    "        if batchNorm:\n",
    "            self.block2 = nn.BatchNorm2d(outputs)\n",
    "        if dropout:\n",
    "            self.block3 = nn.Dropout(0.5)\n",
    "\n",
    "        self.batchNorm = batchNorm\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        if self.batchNorm:\n",
    "            out = self.block2(out)\n",
    "        if self.dropout:\n",
    "            out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, inputs=1):\n",
    "        super(Generator,self).__init__()\n",
    "        \n",
    "        self.d1=  GenBlock(inputs,64,batchNorm=False)\n",
    "        self.d2=  GenBlock(64,128)\n",
    "        self.d3=  GenBlock(128,256)\n",
    "        self.d4=  GenBlock(256,512)\n",
    "        self.d5=  GenBlock(512,512)\n",
    "        self.d6=  GenBlock(512,512)\n",
    "        self.d7=  GenBlock(512,512)\n",
    "        self.d8=  nn.Sequential(nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=False), nn.LeakyReLU(0.2))\n",
    "        \n",
    "        \n",
    "        self.u1 = GenBlock(512,512,False,dropout=True)\n",
    "        self.u2 = GenBlock(1024,512,False,dropout=True)\n",
    "        self.u3 = GenBlock(1024,512,False,dropout=True)\n",
    "        self.u4 = GenBlock(1024,512,False)\n",
    "        self.u5 = GenBlock(1024,256,False)\n",
    "        self.u6 = GenBlock(512,128,False)\n",
    "        self.u7 = GenBlock(256,64,False)\n",
    "        self.u8 = nn.Sequential(nn.ConvTranspose2d(128, 2, kernel_size=4, stride=2, padding=1, bias=False), nn.Tanh())\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        dd1 = self.d1(x)\n",
    "        dd2 = self.d2(dd1)\n",
    "        dd3 = self.d3(dd2)\n",
    "        dd4 = self.d4(dd3)\n",
    "        dd5 = self.d5(dd4)\n",
    "        dd6 = self.d6(dd5)\n",
    "        dd7 = self.d7(dd6)\n",
    "        dd8 = self.d8(dd7)\n",
    "        uu1 = self.u1(dd8)\n",
    "        uu2 = self.u2(torch.concat([uu1,dd7],1)) #Skip Connection from dd7 to uu1\n",
    "        uu3 = self.u3(torch.concat([uu2,dd6],1))\n",
    "        uu4 = self.u4(torch.concat([uu3,dd5],1))\n",
    "        uu5 = self.u5(torch.concat([uu4,dd4],1))\n",
    "        uu6 = self.u6(torch.concat([uu5,dd3],1))\n",
    "        uu7 = self.u7(torch.concat([uu6,dd2],1))\n",
    "        uu8 = self.u8(torch.concat([uu7,dd1],1))\n",
    "        return uu8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGenerator=Generator(1)\n",
    "summary(testGenerator,(1,ImageSize,ImageSize),batch_size=BatchSize,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscBlock(nn.Module):\n",
    "    def __init__(self, inputs, outputs,  kernel=4, stride=2, padding=1, batchNorm=True, activation=True):\n",
    "        super(DiscBlock,self).__init__()\n",
    "        \n",
    "        self.block1 = nn.Conv2d(inputs, outputs, kernel, stride, padding, bias=not batchNorm)\n",
    "        if batchNorm: self.block2 = nn.BatchNorm2d(outputs)\n",
    "        if activation: self.block3 = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "        self.batchNorm = batchNorm\n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        if self.batchNorm:\n",
    "            out = self.block2(out)\n",
    "        if self.activation:\n",
    "            out = self.block3(out)\n",
    "        # print(out.shape)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, inputs=3):\n",
    "        super(Discriminator,self).__init__()\n",
    "\n",
    "        self.b1 = DiscBlock(inputs,64,batchNorm=False)\n",
    "        self.b2 = DiscBlock(64,128)\n",
    "        self.b3 = DiscBlock(128,256)\n",
    "        self.b4 = DiscBlock(256,512,stride=1)\n",
    "        self.b5 = DiscBlock(512,1,stride=1,batchNorm=False,activation=False)\n",
    "                                \n",
    "    def forward(self, x):\n",
    "        #print(x.shape())\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(y1)\n",
    "        y3 = self.b3(y2)\n",
    "        y4 = self.b4(y3)\n",
    "        y5 = self.b5(y4)\n",
    "        return y5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDiscriminator=Discriminator(3)\n",
    "summary(testDiscriminator,(3,ImageSize,ImageSize),batch_size=BatchSize,device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13dd93a277554efb33d9dc9352f7e61f8b4dd00a9187561e5f5d511fdca2ba07"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
